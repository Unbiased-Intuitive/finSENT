{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xc1_R1g1HFgV",
    "outputId": "2fa7020a-fb37-46b9-d10b-9042cd1e0dea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, Dropout,Flatten,GRU\n",
    "\n",
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K \n",
    "\n",
    "import itertools \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "np.random.seed(7)\n",
    "\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "import os, re, csv, math, codecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf9Y8NV2HYir"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n",
    "\n",
    "\n",
    "def BidGRU(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(GRU(300, return_sequences=True, dropout=0.25,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "#    x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def BidGRUNoAtt(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(GRU(300,  dropout=0.25,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    #x = Attention(maxlen)(x)\n",
    "    #x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "  \n",
    "def BidLSTMNoAtt(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(300,  dropout=0.25, return_sequences=True,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    #x = Attention(maxlen)(x)\n",
    "    #x = Flatten(x)\n",
    "    x = TimeDistributed(Dense(256, activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Dropout(0.25))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def BidLstm(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(300, return_sequences=True, dropout=0.25,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "#    x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "def print_cm(y_test,y_pred):\n",
    "    true_test_labels = ['negative','positive']\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=true_test_labels,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=true_test_labels, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def LstmCnn(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(GRU(300, return_sequences=True, dropout=0.25,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "#    x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    inp1 = Input(shape=(512,))\n",
    "    x1 = Embedding(512, 512, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp1)\n",
    "    x1=Conv1D(16, kernel_size=3, activation='elu', padding='same',\n",
    "                             input_shape=(vector_size, 1))(x1)\n",
    "    x1=Dense(512, activation='relu',input_shape=(vector_size, 1))(x1)\n",
    "    x1=Dense(64, activation='relu')(x1)\n",
    "    x1=Dense(8, activation='relu')(x1)\n",
    "    x1=Flatten()(x1)\n",
    "    x1=Dense(2, activation='softmax')(x1)\n",
    "    \n",
    "    model_cnn = Model(inputs=inp1, outputs=x1)\n",
    "\n",
    "    model_cnn.add(Conv1D(16, kernel_size=3, activation='elu', padding='same',\n",
    "                             input_shape=(vector_size, 1)))\n",
    "    model_cnn.add(Dense(512, activation='relu',input_shape=(vector_size, 1)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model_cnn.add(Dense(64, activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    model_cnn.add(Dense(8, activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    model_cnn.add(Flatten())\n",
    "    model_cnn.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    combined_model = Sequential()\n",
    "    combined_model.add(Merge([model, model_cnn], mode='concat', concat_axis=1))\n",
    "\n",
    "    return combined_model\n",
    "\n",
    "def make_df(train_path, test_path, max_features, maxlen, list_classes, word_index):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    train = train.sample(frac=1)\n",
    "\n",
    "    list_sentences_train = train[\"message\"].fillna(\"unknown\").values\n",
    "    y = train[list_classes].values\n",
    "    \n",
    "    y_test = test[list_classes].values\n",
    "    \n",
    "    y=np.where(y == 'Bullish', 1.0, 0.0)\n",
    "    y_test=np.where(y_test == 'Bullish', 1.0, 0.0)\n",
    "    list_sentences_test = test[\"spans\"].fillna(\"unknown\").values\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.word_index = word_index\n",
    "    #tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    #print(list_tokenized_train[0])\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "    X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "    #word_index = tokenizer.word_index\n",
    "    \n",
    "    return X_t, X_te, y, y_test\n",
    "\n",
    "def create_sequence(word_index, sent, maxlen):\n",
    "    token = text.Tokenizer()\n",
    "    token.word_index=word_index\n",
    "    tokenized_text = token.texts_to_sequences(sent)\n",
    "    X_text = sequence.pad_sequences(tokenized_text, maxlen=maxlen)\n",
    "    return X_text\n",
    "\n",
    "def make_glovevec(glovepath, max_features, embed_size):\n",
    "    embedding_matrix = np.zeros((max_features+1, embed_size))\n",
    "    f = open(glovepath, encoding=\"utf8\")\n",
    "    word_index = {}\n",
    "    count=0\n",
    "    for line in f:\n",
    "        count+=1\n",
    "        if count > max_features:\n",
    "            break\n",
    "        else:\n",
    "            values = line.split()\n",
    "            word_index[values[0]]=count\n",
    "            #print(values)\n",
    "            #word = ' '.join(values[:-embed_size])\n",
    "            coefs = np.asarray(values[-embed_size:], dtype='float32')\n",
    "            embedding_matrix[count]=coefs.reshape(-1)\n",
    "        #print(embeddings_index[word])\n",
    "    f.close()\n",
    "    \n",
    "    return embedding_matrix, word_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "colab_type": "code",
    "id": "Ufdh6-mbHh_N",
    "outputId": "9747ba90-0118-49d4-a16e-08c727093a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-07 20:46:59--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 104.22.74.142, 2606:4700:10::6816:4a8e, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681808098 (650M) [application/zip]\n",
      "Saving to: ‘wiki-news-300d-1M.vec.zip’\n",
      "\n",
      "wiki-news-300d-1M.v 100%[===================>] 650.22M  9.69MB/s    in 63s     \n",
      "\n",
      "2020-04-07 20:48:03 (10.4 MB/s) - ‘wiki-news-300d-1M.vec.zip’ saved [681808098/681808098]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "yi7eNRSRH4gE",
    "outputId": "fc65c3f1-276d-4111-eb4f-3902f31e184a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wiki-news-300d-1M.vec.zip\n",
      "  inflating: wiki-news-300d-1M.vec   \n"
     ]
    }
   ],
   "source": [
    "! unzip wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "f0j_i26bdPB9",
    "outputId": "f5614f90-5f54-4bf6-a79c-1e8146c381f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/users/kostadin.mishev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "M1IqzIduHc58",
    "outputId": "07f981a3-ad46-4fa2-faf1-31dfb0f80404"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "677it [00:00, 6768.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [02:28, 6736.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 999995 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#load embeddings\n",
    "\n",
    "from tqdm import tqdm\n",
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/home/users/kostadin.mishev/datasets/fasttext/wiki-news-300d-1M.vec', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "dDZjqjtePf_f",
    "outputId": "c5e4a65e-6e31-41f9-bb3f-f0e33979a4b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num train:  1748\n",
      "num test:  438\n"
     ]
    }
   ],
   "source": [
    "#load \n",
    "test_df = pd.read_csv(r\"/home/users/kostadin.mishev/phd/dataset/dev/dev.tsv\", names=['id','sent','a','text'],sep='\\t')\n",
    "train_df = pd.read_csv(r\"/home/users/kostadin.mishev/phd/dataset/train/train.tsv\", names=['id','sent','a','text'],sep='\\t')\n",
    "test_df = test_df.fillna('_NA_')\n",
    "\n",
    "print(\"num train: \", train_df.shape[0])\n",
    "print(\"num test: \", test_df.shape[0])\n",
    "\n",
    "label_names = [\"sent\"]\n",
    "y_train = train_df[label_names].values\n",
    "y_test = test_df[label_names].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "pRYZtCdBdM7e",
    "outputId": "2359294b-aa5e-46ca-ca29-bc670d636049"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1748/1748 [00:00<00:00, 83224.29it/s]\n",
      "100%|██████████| 438/438 [00:00<00:00, 88043.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing train data...\n",
      "tokenizing input data...\n",
      "dictionary size:  5738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_seq_len=64\n",
    "MAX_NB_WORDS = 100000\n",
    "raw_docs_train = train_df['text'].tolist()\n",
    "raw_docs_test = test_df['text'].tolist() \n",
    "\n",
    "print(\"pre-processing train data...\")\n",
    "processed_docs_train = []\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_train.append(\" \".join(filtered))\n",
    "#end for\n",
    "\n",
    "processed_docs_test = []\n",
    "for doc in tqdm(raw_docs_test):\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    filtered = [word for word in tokens if word not in stop_words]\n",
    "    processed_docs_test.append(\" \".join(filtered))\n",
    "#end for\n",
    "\n",
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nto3HiUNd-eQ"
   },
   "outputs": [],
   "source": [
    "#pad sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "SGrxHfEyeEWl",
    "outputId": "64e1b4fe-4a85-4348-cb30-1f0bab974716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 931\n",
      "sample words not found:  ['bilfinger' 'sal1v' 'breakingviews' 'sneed' 'cumerio' 'affectogenimap'\n",
      " 'maritim' 'mothahir' 'hearst' 'cnc1v']\n"
     ]
    }
   ],
   "source": [
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 300 \n",
    "weight_decay = 1e-4\n",
    "\n",
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = []\n",
    "for sentence in word_seq_train:\n",
    "    sen = [embedding_matrix[w] for w in sentence]\n",
    "    xtrain.append(sen)\n",
    "xtrain = np.asarray(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_mean = np.average(xtrain,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.reshape(xtrain.shape[0],xtrain.shape[1]*xtrain.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1748, 19200)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = []\n",
    "for sentence in word_seq_test:\n",
    "    sen = [embedding_matrix[w] for w in sentence]\n",
    "    xtest.append(sen)\n",
    "xtest = np.asarray(xtest)\n",
    "xtest_mean = np.average(xtest,axis=1)\n",
    "xtest = xtest.reshape(xtest.shape[0],xtest.shape[1]*xtest.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438, 300)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['sent'].values\n",
    "y_test = test_df['sent'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(1748,1)\n",
    "y_test = y_test.reshape(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "203\n",
      "16\n",
      "177\n",
      "0.1751328248846775\n",
      "\n",
      "0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "203\n",
      "16\n",
      "177\n",
      "0.1751328248846775\n",
      "\n",
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "203\n",
      "16\n",
      "177\n",
      "0.1751328248846775\n",
      "\n",
      "0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "203\n",
      "16\n",
      "177\n",
      "0.1751328248846775\n",
      "\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "201\n",
      "18\n",
      "161\n",
      "0.24115646544478436\n",
      "\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "184\n",
      "35\n",
      "85\n",
      "0.46431819480091957\n",
      "\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n",
      "177\n",
      "42\n",
      "53\n",
      "0.5669256408685777\n",
      "\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "172\n",
      "47\n",
      "47\n",
      "0.5707762557077626\n",
      "\n",
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n",
      "173\n",
      "46\n",
      "48\n",
      "0.570800058879536\n",
      "\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "173\n",
      "46\n",
      "51\n",
      "0.5572228726219256\n",
      "\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "174\n",
      "45\n",
      "46\n",
      "0.5844809791708456\n",
      "\n",
      "2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "175\n",
      "44\n",
      "46\n",
      "0.5890656607636812\n",
      "\n",
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "174\n",
      "45\n",
      "46\n",
      "0.5844809791708456\n",
      "\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "175\n",
      "44\n",
      "47\n",
      "0.5845297326412703\n",
      "\n",
      "20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n",
      "175\n",
      "44\n",
      "46\n",
      "0.5890656607636812\n",
      "\n",
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "173\n",
      "46\n",
      "44\n",
      "0.5890656607636812\n",
      "\n",
      "50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "174\n",
      "45\n",
      "45\n",
      "0.589041095890411\n",
      "\n",
      "100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n",
      "174\n",
      "45\n",
      "45\n",
      "0.589041095890411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "for c in [0.0025,0.025,0.1,0.25,1,10,50,100,150,200,1000,2000,5000,10000,20000,30000,50000,100000]:\n",
    "    print(c)\n",
    "    model = SVC(kernel='linear', C=c, gamma=0.001)\n",
    "    model.fit(xtrain_mean, y)\n",
    "    Y_pred = model.predict(xtest_mean)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,Y_pred).ravel() \n",
    "    mcc = matthews_corrcoef(y_test, Y_pred)\n",
    "\n",
    "    print(tp)\n",
    "    print(tn)\n",
    "    print(fp)\n",
    "    print(fn)\n",
    "    print(mcc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#Train the XGboost Model for Classification\n",
    "model1 = xgb.XGBClassifier()\n",
    "model2 = xgb.XGBClassifier(n_estimators=10000, max_depth=256, learning_rate=0.01)\n",
    "\n",
    "xgb_model = model1.fit(xtrain_mean, y)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "Y_pred = xgb_model.predict(xtest_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "167\n",
      "52\n",
      "57\n",
      "0.5024140654787853\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test,Y_pred).ravel() \n",
    "mcc = matthews_corrcoef(y_test, Y_pred)\n",
    "\n",
    "print(tp)\n",
    "print(tn)\n",
    "print(fp)\n",
    "print(fn)\n",
    "print(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-2EcqLBYcrUv",
    "outputId": "68790679-dfbb-4941-c786-3d9b5852a949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1660 samples, validate on 88 samples\n",
      "Epoch 1/50\n",
      "1660/1660 [==============================] - 39s 23ms/step - loss: 0.6773 - acc: 0.5943 - val_loss: 0.6345 - val_acc: 0.6477\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63448, saving model to ./fasttext/FASTTEXT_BidLSTMNoAtt.hdf5\n",
      "Epoch 2/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.5602 - acc: 0.7280 - val_loss: 0.6186 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63448 to 0.61865, saving model to ./fasttext/FASTTEXT_BidLSTMNoAtt.hdf5\n",
      "Epoch 3/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.4649 - acc: 0.7780 - val_loss: 0.5079 - val_acc: 0.7386\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61865 to 0.50786, saving model to ./fasttext/FASTTEXT_BidLSTMNoAtt.hdf5\n",
      "Epoch 4/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.4096 - acc: 0.8193 - val_loss: 0.4923 - val_acc: 0.7727\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.50786 to 0.49227, saving model to ./fasttext/FASTTEXT_BidLSTMNoAtt.hdf5\n",
      "Epoch 5/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.3745 - acc: 0.8292 - val_loss: 0.5319 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49227\n",
      "Epoch 6/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.3389 - acc: 0.8482 - val_loss: 0.4588 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49227 to 0.45884, saving model to ./fasttext/FASTTEXT_BidLSTMNoAtt.hdf5\n",
      "Epoch 7/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.3039 - acc: 0.8702 - val_loss: 0.4318 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.45884 to 0.43184, saving model to ./fasttext/FASTTEXT_BidLSTMNoAtt.hdf5\n",
      "Epoch 8/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.2738 - acc: 0.8834 - val_loss: 0.4764 - val_acc: 0.7727\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.43184\n",
      "Epoch 9/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.4416 - acc: 0.7759 - val_loss: 0.6092 - val_acc: 0.6307\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.43184\n",
      "Epoch 10/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.6023 - acc: 0.7009 - val_loss: 0.6845 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.43184\n",
      "Epoch 11/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.6764 - acc: 0.5973 - val_loss: 0.5844 - val_acc: 0.7045\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.43184\n",
      "Epoch 12/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.6754 - acc: 0.5584 - val_loss: 0.6782 - val_acc: 0.6193\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.43184\n",
      "Epoch 13/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.6268 - acc: 0.6599 - val_loss: 0.5578 - val_acc: 0.7159\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.43184\n",
      "Epoch 14/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.3781 - acc: 0.8316 - val_loss: 0.4197 - val_acc: 0.7955\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.43184 to 0.41971, saving model to ./fasttext/FASTTEXT_BidLSTMNoAtt.hdf5\n",
      "Epoch 15/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.2792 - acc: 0.8780 - val_loss: 0.4457 - val_acc: 0.7841\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.41971\n",
      "Epoch 16/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.2203 - acc: 0.9021 - val_loss: 0.4744 - val_acc: 0.8409\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.41971\n",
      "Epoch 17/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.2070 - acc: 0.9108 - val_loss: 0.5702 - val_acc: 0.7955\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.41971\n",
      "Epoch 18/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1826 - acc: 0.9247 - val_loss: 0.4711 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.41971\n",
      "Epoch 19/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1653 - acc: 0.9337 - val_loss: 0.4893 - val_acc: 0.8295\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.41971\n",
      "Epoch 20/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1885 - acc: 0.9232 - val_loss: 0.4921 - val_acc: 0.7841\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.41971\n",
      "Epoch 21/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1610 - acc: 0.9322 - val_loss: 0.8383 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.41971\n",
      "Epoch 22/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1955 - acc: 0.9184 - val_loss: 0.5392 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.41971\n",
      "Epoch 23/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1511 - acc: 0.9346 - val_loss: 0.4703 - val_acc: 0.8466\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.41971\n",
      "Epoch 24/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1280 - acc: 0.9458 - val_loss: 0.5435 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.41971\n",
      "Epoch 25/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1282 - acc: 0.9482 - val_loss: 0.7044 - val_acc: 0.8011\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.41971\n",
      "Epoch 26/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1246 - acc: 0.9485 - val_loss: 0.5777 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.41971\n",
      "Epoch 27/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1032 - acc: 0.9575 - val_loss: 0.5428 - val_acc: 0.8182\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.41971\n",
      "Epoch 28/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1078 - acc: 0.9572 - val_loss: 0.7305 - val_acc: 0.7898\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.41971\n",
      "Epoch 29/50\n",
      "1660/1660 [==============================] - 35s 21ms/step - loss: 0.1043 - acc: 0.9584 - val_loss: 0.6390 - val_acc: 0.7841\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.41971\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "model_rnn = BidLstm(max_seq_len, nb_words, embed_dim, embedding_matrix)\n",
    "\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "file_path = \"./fasttext/FASTTEXT_BidLSTMNoAtt.hdf5\"\n",
    "ckpt = ModelCheckpoint(file_path, monitor='val_loss', verbose=1,\n",
    "                       save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=15)\n",
    "\n",
    "y_train_rnn = keras.utils.to_categorical(y_train, 2)\n",
    "history = model_rnn.fit(word_seq_train, y_train_rnn, batch_size=15, epochs=50, validation_split=0.05, callbacks=[ckpt, early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.load_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,   50,   32,   44, 1361, 1362,    7,  575,  253], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_seq_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3K2x4034es6k"
   },
   "outputs": [],
   "source": [
    "y_pred = model_rnn.predict(word_seq_test)\n",
    "y_pred_labels=np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2UQ-wysPwj8"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./results_BiLSTM+FastText+noAtt.pickle\",\"wb\") as f:\n",
    "    pickle.dump(y_pred_labels,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "3fjHiTugPxuf",
    "outputId": "7a1d0546-5236-4db5-d8f2-320e84ceb405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "0.8509615384615384\n",
      "0.8082191780821918\n",
      "0.82903981264637\n",
      "0.6675092223130027\n",
      "[[188  31]\n",
      " [ 42 177]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_labels))\n",
    "print(precision_score(y_test, y_pred_labels))\n",
    "print(recall_score(y_test, y_pred_labels))\n",
    "print(f1_score(y_test, y_pred_labels))\n",
    "print(matthews_corrcoef(y_test, y_pred_labels))\n",
    "\n",
    "#FP = confusion_matrix(y_test, y_pred_labels).sum(axis=0) - np.diag(confusion_matrix)  \n",
    "#FN = confusion_matrix(y_test, y_pred_labels).sum(axis=1) - np.diag(confusion_matrix)\n",
    "#TP = np.diag(confusion_matrix(y_test, y_pred_labels))\n",
    "#TN = confusion_matrix(y_test, y_pred_labels).values.sum() - (FP + FN + TP)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_labels))\n",
    "#print(FN)\n",
    "#print(TP)\n",
    "#print(TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzzrJiMNjkvx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FinSent_survey-FastText.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
