{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "colab_type": "code",
    "id": "xc1_R1g1HFgV",
    "outputId": "38ba1f97-c29f-40da-c2a1-73a2255c49e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/users/kostadin.mishev/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, Dropout,Flatten,GRU\n",
    "\n",
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K \n",
    "\n",
    "import itertools \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "np.random.seed(7)\n",
    "\n",
    "from keras.layers import TimeDistributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf9Y8NV2HYir"
   },
   "outputs": [],
   "source": [
    "weight_decay = 1e-4\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim, \n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "      \n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n",
    "def CNN(maxlen, max_features, embed_size, embedding_matrix,num_filters=5):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False))\n",
    "  model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "  model.add(MaxPooling1D(2))\n",
    "  model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "  model.add(GlobalMaxPooling1D())\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "  model.add(Dense(2, activation='sigmoid'))\n",
    "  return model;\n",
    "\n",
    "def BidGRU(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True, dropout=0.25,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "#    x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def UniGRU(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = GRU(100, return_sequences=True, dropout=0.25,\n",
    "                           recurrent_dropout=0.25)(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "#    x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def BidGRUNoAtt(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(GRU(300,  dropout=0.25,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    #x = Attention(maxlen)(x)\n",
    "    #x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def UniGRUNoAtt(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = GRU(300,  dropout=0.25,\n",
    "                           recurrent_dropout=0.25)(x)\n",
    "    #x = Attention(maxlen)(x)\n",
    "    #x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def UniLSTMNoAtt(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = LSTM(300,  dropout=0.25, return_sequences=True,\n",
    "                           recurrent_dropout=0.25)(x)\n",
    "    #x = Attention(maxlen)(x)\n",
    "    #x = Flatten(x)\n",
    "    x = TimeDistributed(Dense(256, activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Dropout(0.25))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "  \n",
    "def BidLSTMNoAtt(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(300,  dropout=0.25, return_sequences=True,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    #x = Attention(maxlen)(x)\n",
    "    #x = Flatten(x)\n",
    "    x = TimeDistributed(Dense(256, activation=\"relu\"))(x)\n",
    "    x = TimeDistributed(Dropout(0.25))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def UniLstm(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = LSTM(300, return_sequences=True, dropout=0.25,\n",
    "                           recurrent_dropout=0.25)(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "#    x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "def BidLstm(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(300, return_sequences=True, dropout=0.25,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "#    x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(2, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "def print_cm(y_test,y_pred):\n",
    "    true_test_labels = ['negative','positive']\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=true_test_labels,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=true_test_labels, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def LstmCnn(maxlen, max_features, embed_size, embedding_matrix):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp)\n",
    "    x = Bidirectional(GRU(300, return_sequences=True, dropout=0.25,\n",
    "                           recurrent_dropout=0.25))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "#    x = Flatten(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    inp1 = Input(shape=(512,))\n",
    "    x1 = Embedding(512, 512, weights=[embedding_matrix],\n",
    "                  trainable=False)(inp1)\n",
    "    x1=Conv1D(16, kernel_size=3, activation='elu', padding='same',\n",
    "                             input_shape=(vector_size, 1))(x1)\n",
    "    x1=Dense(512, activation='relu',input_shape=(vector_size, 1))(x1)\n",
    "    x1=Dense(64, activation='relu')(x1)\n",
    "    x1=Dense(8, activation='relu')(x1)\n",
    "    x1=Flatten()(x1)\n",
    "    x1=Dense(2, activation='softmax')(x1)\n",
    "    \n",
    "    model_cnn = Model(inputs=inp1, outputs=x1)\n",
    "\n",
    "    model_cnn.add(Conv1D(16, kernel_size=3, activation='elu', padding='same',\n",
    "                             input_shape=(vector_size, 1)))\n",
    "    model_cnn.add(Dense(512, activation='relu',input_shape=(vector_size, 1)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model_cnn.add(Dense(64, activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    model_cnn.add(Dense(8, activation='relu'))\n",
    "    #model.add(Dropout(0.25))\n",
    "    model_cnn.add(Flatten())\n",
    "    model_cnn.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    combined_model = Sequential()\n",
    "    combined_model.add(Merge([model, model_cnn], mode='concat', concat_axis=1))\n",
    "\n",
    "    return combined_model\n",
    "\n",
    "def make_df(train_path, test_path, max_features, maxlen, list_classes, word_index):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    train = train.sample(frac=1)\n",
    "\n",
    "    list_sentences_train = train[\"message\"].fillna(\"unknown\").values\n",
    "    y = train[list_classes].values\n",
    "    \n",
    "    y_test = test[list_classes].values\n",
    "    \n",
    "    y=np.where(y == 'Bullish', 1.0, 0.0)\n",
    "    y_test=np.where(y_test == 'Bullish', 1.0, 0.0)\n",
    "    list_sentences_test = test[\"spans\"].fillna(\"unknown\").values\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.word_index = word_index\n",
    "    #tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    #print(list_tokenized_train[0])\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "    X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "    #word_index = tokenizer.word_index\n",
    "    \n",
    "    return X_t, X_te, y, y_test\n",
    "\n",
    "def create_sequence(word_index, sent, maxlen):\n",
    "    token = text.Tokenizer()\n",
    "    token.word_index=word_index\n",
    "    tokenized_text = token.texts_to_sequences(sent)\n",
    "    X_text = sequence.pad_sequences(tokenized_text, maxlen=maxlen)\n",
    "    return X_text\n",
    "\n",
    "def make_glovevec(glovepath, max_features, embed_size):\n",
    "    embedding_matrix = np.zeros((max_features+1, embed_size))\n",
    "    f = open(glovepath, encoding=\"utf8\")\n",
    "    word_index = {}\n",
    "    count=0\n",
    "    for line in f:\n",
    "        count+=1\n",
    "        if count > max_features:\n",
    "            break\n",
    "        else:\n",
    "            values = line.split()\n",
    "            word_index[values[0]]=count\n",
    "            #print(values)\n",
    "            #word = ' '.join(values[:-embed_size])\n",
    "            coefs = np.asarray(values[-embed_size:], dtype='float32')\n",
    "            embedding_matrix[count]=coefs.reshape(-1)\n",
    "        #print(embeddings_index[word])\n",
    "    f.close()\n",
    "    \n",
    "    return embedding_matrix, word_index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "Ufdh6-mbHh_N",
    "outputId": "a30b67b5-8ce8-4abd-867d-5554def28638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-08 07:41:34--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2020-04-08 07:41:35--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2020-04-08 07:41:36--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  2.03MB/s    in 6m 58s  \n",
      "\n",
      "2020-04-08 07:48:34 (1.97 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "yi7eNRSRH4gE",
    "outputId": "04f7dc4f-eb75-4ebf-fad7-727a6b221934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0j_i26bdPB9"
   },
   "outputs": [],
   "source": [
    "!mkdir finsent_survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SX12DAvAOUPH"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/phd/github/datascience/datasets/LASER_emb/X_train.raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-548c481fe77c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"D:/phd/github/datascience/datasets/LASER_emb/X_train.raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/phd/github/datascience/datasets/LASER_emb/X_train.raw'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "\n",
    "x_train = np.fromfile(\"D:/phd/github/datascience/datasets/LASER_emb/X_train.raw\", dtype=np.float32, count=-1)\n",
    "dim = 1024\n",
    "din = 1024\n",
    "x_train.resize(x_train.shape[0] // dim, din) \n",
    "X_test = np.fromfile(\"D:/phd/github/datascience/datasets/LASER_emb/X_test.raw\", dtype=np.float32, count=-1)\n",
    "X_test.resize(X_test.shape[0] // dim, din)  \n",
    "\n",
    "\n",
    "model_lstm = UniGRUNoAtt(maxlen, len(word_index)+1, embed_size, embedding_vector)\n",
    "\n",
    "file_path = \"./finsent_survey/model_CNN_extended.hdf5\"\n",
    "ckpt = ModelCheckpoint(file_path, monitor='val_loss', verbose=1,\n",
    "                       save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=50)\n",
    "\n",
    "history = model_full.fit(x_train, y, batch_size=32, epochs=40, validation_split=0.1,callbacks=[ckpt, early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "3fjHiTugPxuf",
    "outputId": "0374f918-7505-4985-cdf8-d558c7d93cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.620442319187089\n",
      "0.7372793354101765\n",
      "0.6501831501831502\n",
      "0.6909975669099757\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred_labels))\n",
    "print(precision_score(y_test, y_pred_labels))\n",
    "print(recall_score(y_test, y_pred_labels))\n",
    "print(f1_score(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "LzzrJiMNjkvx",
    "outputId": "711d1d8a-bc65-42ef-a567-10c847936736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6401673640167364\n",
      "0.7179715302491103\n",
      "0.739010989010989\n",
      "0.7283393501805054\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred_labels))\n",
    "print(precision_score(y_test, y_pred_labels))\n",
    "print(recall_score(y_test, y_pred_labels))\n",
    "print(f1_score(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jTpuMAuZdkHx"
   },
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "maxlen = 64\n",
    "embed_size = 300\n",
    "list_classes = [\"sentiment\"]\n",
    "embedding_vector, word_index = make_glovevec(\"/home/users/kostadin.mishev/datasets/glove/glove.6B.300d.txt\",\n",
    "                                 max_features, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cjprpsfmc_ib"
   },
   "outputs": [],
   "source": [
    "def make_df(train_path, test_path, max_features, maxlen, list_classes, word_index):\n",
    "    train = pd.read_csv(train_path,names=[\"id\",\"sentiment\",\"a\",\"message\"],sep='\\t')\n",
    "    test = pd.read_csv(test_path,names=[\"id\",\"sentiment\",\"a\",\"spans\"],sep='\\t')\n",
    "    #train = train.sample(frac=1)\n",
    "\n",
    "    list_sentences_train = train[\"message\"].fillna(\"unknown\").values\n",
    "    y = train[list_classes].values\n",
    "    \n",
    "    y_test = test[list_classes].values\n",
    "    \n",
    "    #y=np.where(y == 'Bullish', 1.0, 0.0)\n",
    "    #y_test=np.where(y_test == 'Bullish', 1.0, 0.0)\n",
    "    list_sentences_test = test[\"spans\"].fillna(\"unknown\").values\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "    tokenizer.word_index = word_index\n",
    "    #tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    #print(list_tokenized_train[0])\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "    X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "    #word_index = tokenizer.word_index\n",
    "    \n",
    "    return X_t, X_te, y, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPEcMbrQcSP2"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "xtr, xte, y, y_test= make_df(r\"/home/users/kostadin.mishev/phd/dataset/train/train.tsv\",\n",
    "                                  r\"/home/users/kostadin.mishev/phd/dataset/dev/dev.tsv\",\n",
    "                                  max_features, maxlen, list_classes, word_index)\n",
    "y = keras.utils.to_categorical(y, 2)\n",
    "y_test=keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = []\n",
    "for sentence in xtr:\n",
    "    sen = [embedding_vector[w] for w in sentence]\n",
    "    xtrain.append(sen)\n",
    "xtrain_mean = np.average(xtrain,axis=1)\n",
    "xtrain = np.asarray(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "iZR__kGO-meI",
    "outputId": "65198728-c3b9-46af-f41a-488305766370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1748, 19200)\n"
     ]
    }
   ],
   "source": [
    "xtrain = xtrain.reshape(xtrain.shape[0],xtrain.shape[1]*xtrain.shape[2])\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = []\n",
    "for sentence in xte:\n",
    "    sen = [embedding_vector[w] for w in sentence]\n",
    "    xtest.append(sen)\n",
    "xtest = np.asarray(xtest)\n",
    "xtest_mean = np.average(xtest,axis=1)\n",
    "xtest = xtest.reshape(xtest.shape[0],xtest.shape[1]*xtest.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.argmax(y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025\n",
      "52\n",
      "192\n",
      "27\n",
      "167\n",
      "0.14844967076245041\n",
      "\n",
      "0.025\n",
      "66\n",
      "186\n",
      "33\n",
      "153\n",
      "0.18013433204450768\n",
      "\n",
      "0.1\n",
      "72\n",
      "195\n",
      "24\n",
      "147\n",
      "0.26490647141300877\n",
      "\n",
      "0.25\n",
      "106\n",
      "203\n",
      "16\n",
      "113\n",
      "0.45837302613186937\n",
      "\n",
      "1\n",
      "148\n",
      "183\n",
      "36\n",
      "71\n",
      "0.5180745433377699\n",
      "\n",
      "10\n",
      "175\n",
      "178\n",
      "41\n",
      "44\n",
      "0.6119295638588299\n",
      "\n",
      "50\n",
      "177\n",
      "179\n",
      "40\n",
      "42\n",
      "0.6255968645319715\n",
      "\n",
      "100\n",
      "177\n",
      "176\n",
      "43\n",
      "42\n",
      "0.611878525069479\n",
      "\n",
      "150\n",
      "177\n",
      "177\n",
      "42\n",
      "42\n",
      "0.6164383561643836\n",
      "\n",
      "200\n",
      "176\n",
      "177\n",
      "42\n",
      "43\n",
      "0.611878525069479\n",
      "\n",
      "1000\n",
      "176\n",
      "178\n",
      "41\n",
      "43\n",
      "0.6164640635898989\n",
      "\n",
      "2000\n",
      "174\n",
      "179\n",
      "40\n",
      "45\n",
      "0.6120316797650658\n",
      "\n",
      "5000\n",
      "174\n",
      "179\n",
      "40\n",
      "45\n",
      "0.6120316797650658\n",
      "\n",
      "10000\n",
      "172\n",
      "179\n",
      "40\n",
      "47\n",
      "0.6030478607243883\n",
      "\n",
      "20000\n",
      "170\n",
      "181\n",
      "38\n",
      "49\n",
      "0.6035014886665504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "for c in [0.0025,0.025,0.1,0.25,1,10,50,100,150,200,1000,2000,5000,10000,20000]:\n",
    "    print(c)\n",
    "    model = SVC(kernel='linear', C=c, gamma=0.001)\n",
    "    model.fit(xtrain_mean, y)\n",
    "    Y_pred = model.predict(xtest_mean)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test,Y_pred).ravel() \n",
    "    mcc = matthews_corrcoef(y_test, Y_pred)\n",
    "\n",
    "    print(tp)\n",
    "    print(tn)\n",
    "    print(fp)\n",
    "    print(fn)\n",
    "    print(mcc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "174\n",
      "45\n",
      "68\n",
      "0.4867098623751561\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#Train the XGboost Model for Classification\n",
    "model1 = xgb.XGBClassifier()\n",
    "#model2 = xgb.XGBClassifier(n_estimators=10000, max_depth=256, learning_rate=0.01)\n",
    "\n",
    "xgb_model = model1.fit(xtrain_mean, y)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "Y_pred = xgb_model.predict(xtest_mean)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,Y_pred).ravel() \n",
    "mcc = matthews_corrcoef(y_test, Y_pred)\n",
    "\n",
    "print(tp)\n",
    "print(tn)\n",
    "print(fp)\n",
    "print(fn)\n",
    "print(mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'n_splits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-92be43174ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n\u001b[0;32m---> 22\u001b[0;31m                    \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                    \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                    verbose=2, refit=True)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'n_splits'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "#from sklearn.cross_validation import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import *\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05], #so called `eta` value\n",
    "              'max_depth': [6],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [5], #number of trees, change it to 1000 for better results\n",
    "              'missing':[-999],\n",
    "              'seed': [1337]}\n",
    "\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters, n_jobs=5, \n",
    "                   cv=StratifiedKFold(y, n_splits=5, shuffle=True), \n",
    "                   scoring='roc_auc',\n",
    "                   verbose=2, refit=True)\n",
    "\n",
    "clf.fit(xtrain_mean, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "colab_type": "code",
    "id": "JgkrNwW6ctuY",
    "outputId": "b946bba5-8156-4efc-adea-5f00ebee4609"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1748 samples, validate on 438 samples\n",
      "Epoch 1/50\n",
      "1748/1748 [==============================] - 2s 1ms/step - loss: 0.6987 - acc: 0.5014 - val_loss: 0.6936 - val_acc: 0.5080\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69361, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 2/50\n",
      "1748/1748 [==============================] - 0s 83us/step - loss: 0.6927 - acc: 0.4997 - val_loss: 0.6937 - val_acc: 0.5228\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.69361\n",
      "Epoch 3/50\n",
      "1748/1748 [==============================] - 0s 81us/step - loss: 0.6904 - acc: 0.5132 - val_loss: 0.6921 - val_acc: 0.5525\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69361 to 0.69212, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 4/50\n",
      "1748/1748 [==============================] - 0s 78us/step - loss: 0.6826 - acc: 0.5398 - val_loss: 0.6856 - val_acc: 0.5662\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.69212 to 0.68562, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 5/50\n",
      "1748/1748 [==============================] - 0s 84us/step - loss: 0.6686 - acc: 0.5518 - val_loss: 0.6742 - val_acc: 0.6084\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.68562 to 0.67425, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 6/50\n",
      "1748/1748 [==============================] - 0s 80us/step - loss: 0.6377 - acc: 0.6141 - val_loss: 0.6459 - val_acc: 0.6199\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.67425 to 0.64588, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 7/50\n",
      "1748/1748 [==============================] - 0s 77us/step - loss: 0.5995 - acc: 0.7085 - val_loss: 0.6182 - val_acc: 0.6963\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.64588 to 0.61822, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 8/50\n",
      "1748/1748 [==============================] - 0s 82us/step - loss: 0.5624 - acc: 0.7574 - val_loss: 0.5899 - val_acc: 0.7249\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61822 to 0.58993, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 9/50\n",
      "1748/1748 [==============================] - 0s 73us/step - loss: 0.5247 - acc: 0.8106 - val_loss: 0.5718 - val_acc: 0.7717\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.58993 to 0.57182, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 10/50\n",
      "1748/1748 [==============================] - 0s 82us/step - loss: 0.4872 - acc: 0.8547 - val_loss: 0.5507 - val_acc: 0.7660\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.57182 to 0.55065, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 11/50\n",
      "1748/1748 [==============================] - 0s 78us/step - loss: 0.4532 - acc: 0.8819 - val_loss: 0.5336 - val_acc: 0.7626\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.55065 to 0.53359, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 12/50\n",
      "1748/1748 [==============================] - 0s 73us/step - loss: 0.4002 - acc: 0.9119 - val_loss: 0.5248 - val_acc: 0.7683\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.53359 to 0.52483, saving model to /home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\n",
      "Epoch 13/50\n",
      "1748/1748 [==============================] - 0s 76us/step - loss: 0.3810 - acc: 0.9199 - val_loss: 0.5255 - val_acc: 0.7728\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.52483\n",
      "Epoch 14/50\n",
      "1748/1748 [==============================] - 0s 78us/step - loss: 0.3396 - acc: 0.9354 - val_loss: 0.5451 - val_acc: 0.7637\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.52483\n",
      "Epoch 15/50\n",
      "1748/1748 [==============================] - 0s 80us/step - loss: 0.3217 - acc: 0.9365 - val_loss: 0.5693 - val_acc: 0.7705\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.52483\n",
      "Epoch 16/50\n",
      "1748/1748 [==============================] - 0s 77us/step - loss: 0.2871 - acc: 0.9511 - val_loss: 0.5523 - val_acc: 0.7820\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.52483\n",
      "Epoch 17/50\n",
      "1748/1748 [==============================] - 0s 76us/step - loss: 0.2565 - acc: 0.9640 - val_loss: 0.5626 - val_acc: 0.7751\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.52483\n"
     ]
    }
   ],
   "source": [
    "model_lstm = CNN(maxlen, len(word_index)+1, embed_size, embedding_vector)\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "file_path = \"/home/users/kostadin.mishev/phd/finsent/glove/model_BidGRU_NoAtt.hdf5\"\n",
    "ckpt = ModelCheckpoint(file_path, monitor='val_loss', verbose=1,\n",
    "                       save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model_lstm, show_shapes=True, to_file='model.png')\n",
    "\n",
    "history = model_lstm.fit(xtr, y, batch_size=120, epochs=50, validation_data=(xte,y_test), callbacks=[ckpt, early])\n",
    "y_pred=model_lstm.predict(xte)\n",
    "y_pred_labels=np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMJgtePCdCr-"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./results_BidGRU+GLOVE300.pickle\",\"wb\") as f:\n",
    "    pickle.dump(y_pred_labels,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.load_weights(filepath=file_path)\n",
    "\n",
    "y_pred=model_lstm.predict(xte)\n",
    "y_pred_labels=np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769406392694064\n",
      "0.8172043010752689\n",
      "0.6940639269406392\n",
      "0.7506172839506172\n",
      "0.5450360955491317\n",
      "[[185  34]\n",
      " [ 67 152]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "y_test_labels = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(accuracy_score(y_test_labels, y_pred_labels))\n",
    "print(precision_score(y_test_labels, y_pred_labels))\n",
    "print(recall_score(y_test_labels, y_pred_labels))\n",
    "print(f1_score(y_test_labels, y_pred_labels))\n",
    "print(matthews_corrcoef(y_test_labels, y_pred_labels))\n",
    "\n",
    "#FP = confusion_matrix(y_test, y_pred_labels).sum(axis=0) - np.diag(confusion_matrix)  \n",
    "#FN = confusion_matrix(y_test, y_pred_labels).sum(axis=1) - np.diag(confusion_matrix)\n",
    "#TP = np.diag(confusion_matrix(y_test, y_pred_labels))\n",
    "#TN = confusion_matrix(y_test, y_pred_labels).values.sum() - (FP + FN + TP)\n",
    "\n",
    "print(confusion_matrix(y_test_labels, y_pred_labels))\n",
    "#print(FN)\n",
    "#print(TP)\n",
    "#print(TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5So3gEE1fVz2"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "x = Embedding(len(word_index)+1, embed_size, weights=[embedding_vector],\n",
    "              trainable=False)(inp)\n",
    "x = Bidirectional(GRU(100,weights=model_lstm.layers[2].get_weights(), return_sequences=True, dropout=0.25,\n",
    "                        recurrent_dropout=0.25))(x)\n",
    "x = Attention(maxlen,weights=model_lstm.layers[3].get_weights())(x)\n",
    "#    x = Flatten(x)\n",
    "x = Dense(256, activation=\"relu\",weights=model_lstm.layers[4].get_weights())(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "x_train_emb = model.predict(xte)\n",
    "\n",
    "#x = Attention(maxlen)(x)\n",
    "\n",
    "#x = Dense(256, activation=\"relu\",weights=model_lstm.layers[4].get_weights())(x)\n",
    "#x = Dropout(0.25)(x)\n",
    "#x = Dense(2, activation=\"sigmoid\")(x)\n",
    "#model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "#def BidGRU(maxlen, max_features, embed_size, embedding_matrix)\n",
    "#model = BidGRU(maxlen, len(word_index)+1, embed_size, embedding_vector)\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "#activations = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kLG0qIiRKhP2",
    "outputId": "03e48e30-114b-4062-d56e-83f6cc46fd8b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-de3c8368e31c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train_emb' is not defined"
     ]
    }
   ],
   "source": [
    "x_train_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWB3bjSCM4w2"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"test.pickle\",\"wb\") as f:\n",
    "    pickle.dump(x_train_emb,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgjpbE0pNHnu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FinSent_survey-Glove.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
